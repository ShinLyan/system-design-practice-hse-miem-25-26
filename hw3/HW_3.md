# Домашнее задание №3
## Кратко про Patroni Postgresql High Availability Cluster
В прошлом задании мы ломали Kafka. Теперь пришло время базы данных. Обычный PostgreSQL - это надежная СУБД, но у нее есть одна проблема: Single Point of Failure (SPOF) ака единая точка отказа. Если сервер с базой падает, ваш прод лежит, пока админ не проснется и не поднимет бэкап или не переключит на реплику вручную.

Иногда переключение должно происходить автоматически и быстро. Для этого мы используем стек: Patroni + DCS (etcd) + HAProxy.

etcd является DCS (Distributed Configuration Store). Это key-value хранилище. Здесь хранится информация: кто сейчас лидер, какое сейчас состояние кластера, какие ноды в него входят? etcd работает на алгоритме Raft (как и Kafka KRaft из прошлого ДЗ), поэтому ему тоже нужен кворум.

Patroni (template для управления PostgreSQL). Это ПО, написанное на Python, располагается на каждом сервере рядом с PostgreSQL. Он мониторит здоровье базы, общается с etcd. Все экзмепляры Patroni борятся за лидерский ключ (Leader Key) в etcd. Кто держит ключ, тот и лидер (Master). У кого ключа нет, тот переключает базу в режим реплики.

HAProxy. Балансировщик нагрузки. Клиентское приложение не знает IP-адреса конкретных серверов БД. Оно стучится в один адрес HAProxy. HAProxy постоянно опрашивает Patroni (через REST API): кто лидер, кто реплики? И перенаправляет трафик приложения на актуального лидера.

## Суть
Хочется, чтобы мы закрепили тему отказоустойчивости кластера PostgreSQL, потрогали DCS (в нашем случае etcd) и поняли, как здесь работает haproxy и для чего он нужен.

Первым шагом из директории `patroni-master` выполните команду (чтобы сбилдить образ patroni)
```bash
docker build -t patroni .
```

Затем, если образ собрался без ошибок, выполните команду из директории `hw3`
```bash
docker compose up -d
```

Patroni имеет свою CLI утилиту. Зайдите в любой контейнер patroni, сделайте скриншот и объясните, из чего состоит кластер.

```bash
patronictl list
```

Зайдите на URL, посмотрите состояние HAProxy, сделайте скриншот и напишите выводы.

```bash
http://localhost:7001/
```

Подключитесь к мастер-ноде (порт 5001) и репликам (порт 5002) через что-то для просмотра (Dbeaver / psql).
Требуется пролить следующий скрипт в базу:

```sql
-- Создание таблицы справочника владельцев
CREATE TABLE owners (
    id SERIAL PRIMARY KEY,
    owner_name VARCHAR(100) NOT NULL UNIQUE
);

-- Создание таблицы событий
CREATE TABLE events (
    id SERIAL PRIMARY KEY,
    event_name VARCHAR(200) NOT NULL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    owner_name VARCHAR(100) NOT NULL,
    
    -- Внешний ключ для связи с таблицей owners
    CONSTRAINT fk_events_owners 
        FOREIGN KEY (owner_name) 
        REFERENCES owners(owner_name)
        ON DELETE RESTRICT
        ON UPDATE CASCADE
);

-- Создание индексов для оптимизации запросов
CREATE INDEX idx_events_timestamp ON events(timestamp);
CREATE INDEX idx_events_owner_name ON events(owner_name);
CREATE INDEX idx_owners_name ON owners(owner_name);

-- Комментарии к таблицам и полям
COMMENT ON TABLE owners IS 'Справочник владельцев событий';
COMMENT ON COLUMN owners.id IS 'Уникальный идентификатор владельца';
COMMENT ON COLUMN owners.owner_name IS 'Имя владельца (уникальное)';

COMMENT ON TABLE events IS 'Таблица событий';
COMMENT ON COLUMN events.id IS 'Уникальный идентификатор события';
COMMENT ON COLUMN events.event_name IS 'Название события';
COMMENT ON COLUMN events.timestamp IS 'Временная метка события';
COMMENT ON COLUMN events.owner_name IS 'Имя владельца события (ссылка на owners.owner_name)';

-- Добавление владельцев
INSERT INTO owners (owner_name) VALUES 
    ('Иван Петров'),
    ('Мария Сидорова'),
    ('Алексей Козлов');

-- Добавление событий
INSERT INTO events (event_name, owner_name) VALUES 
    ('Встреча с клиентом', 'Иван Петров'),
    ('Презентация проекта', 'Мария Сидорова');
```

Запустите скрипт-стрелялку traffic-generator.py, предварительно установив локально psycopg2

```bash
pip3 install psycopg2
python3 traffic-generator.py
```

Если скрипт запустится корректно, вы увидете новые записи в БД и в stdout будут логи об успешных чтениях/записи.

Наблюдайте: пишется/читается? Откуда пишется, откуда читается?

Не останавливая скрипт, попробуйте поиграться с etcd и patroni.
Выключайте контейнеры, включайте их и наблюдайте, что происходит с вашим кластером и приложением.

    * Продолжает ли приложение работать?
    * Что меняется, если выключаешь/включаешь ноду patroni? Что если выключить лидера, что если выключить реплику? Как меняется состояние кластера?
    * Что меняется, если выключаешь/включаешь ноду etcd? Продолжает ли приложение читать/писать данные?
    * Что если выключить haproxy? Достаточная ли у нас отказоустойчивость кластера? Как избежать в продакшен решении этого кейса?

Также, я буду признателен, если посмотрите по итогам в Grafana (там уже будут 3 готовых дашборда). Если их там нет, возьмите дашборды из директории grafana_dashboards и импортируйте json в Grafana по адресу http://localhost:3000 (admin/admin).

Эту часть отразите в отчете в свободном виде.

## Как сдавать
1. Пришлите в телеграм @nikolaysavelev ссылку на ваш репозиторий
2. В репозитории должен быть проект, в директории hw3 должен быть файл HW_3_SOLUTION.md. Формат отчета свободный.

## Критерии оценки
1. Если смогли запустить кластер - уже отлично!
2. Если смогли пострелять - вообще супер!
3. Если смогли проанализировать и понять как работает кластер, поиграться с отказоустойчивостью - спасибо большое, высший балл!