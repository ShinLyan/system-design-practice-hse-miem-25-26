# HW2 – Apache Kafka. Масштабируемость и отказоустойчивость

Шин В.А. МКС244

## 1. Архитектура решения

В рамках домашнего задания был развернут Kafka-кластер из трёх брокеров (`kafka01`, `kafka02`, `kafka03`) в Docker Compose.  
Kafka запущена в режиме **KRaft**, без использования ZooKeeper.

Каждый брокер:
- хранит данные (партиции и реплики)
- участвует в выборе контроллера кластера
- может выполнять роль лидера или реплики партиций

Kafka в данной работе используется как распределённый лог для асинхронной передачи событий между продюсером и кластером.

---

## 2. Конфигурация топиков

### 2.1. Топик `test_topic`

Топик был создан со следующими параметрами:
- **Partitions: 8**
- **Replication factor: 3**

#### Обоснование конфигурации

- **8 партиций**  
  Позволяют масштабировать throughput за счёт параллельной записи. Kafka масштабируется именно через партиции, а не через количество брокеров.

- **Replication factor = 3**  
  Каждая партиция хранится на всех трёх брокерах, что обеспечивает:
  - отказоустойчивость
  - сохранность данных при падении одного брокера

При такой конфигурации кластер устойчив к отказу одного узла и теряет работоспособность при отказе двух, так как пропадает кворум.

---

## 3. Нагрузочное тестирование Kafka

Для проверки пропускной способности был использован стандартный инструмент `kafka-producer-perf-test`.

В кластер было отправлено **1 000 000 сообщений** со следующими параметрами:
- размер сообщения: 1000 байт
- отправка батчами
- асинхронная запись (`acks=1`)
- без ограничения throughput

### Наблюдения

- Kafka стабильно принимает все сообщения без ошибок
- Сообщения равномерно распределяются по партициям
- Данные присутствуют на всех брокерах за счёт репликации
- Высокий throughput достигается за счёт:
  - батчевой отправки
  - параллельной записи в партиции
  - отсутствия блокировок между продюсером и консьюмерами

Данный тест подтверждает, что Kafka хорошо подходит для сценариев с высокой нагрузкой на запись.

---

## 4. Проверка отказоустойчивости

### 4.1. Базовая проверка

Был запущен Python-скрипт `example.py`, который каждые 0.5 секунды отправляет события в топик `critical_data`.

При полностью рабочем кластере:
- сообщения стабильно записываются
- Kafka UI показывает рост оффсетов
- ошибок со стороны продюсера нет

---

### 4.2. Остановка контроллера

После остановки брокера, выполнявшего роль контроллера:
- Kafka автоматически выбирает нового контроллера
- кластер остаётся доступным
- скрипт `example.py` продолжает отправлять сообщения без ошибок

Это демонстрирует отказоустойчивость кластера при отказе одного брокера.

---

### 4.3. Остановка второго брокера (2 из 3)

После остановки второго брокера:
- кластер теряет кворум
- запись сообщений прекращается
- продюсер начинает получать ошибки

Причина:
- для работы KRaft требуется большинство узлов
- при 1 живом брокере из 3 невозможно поддерживать консистентность метаданных
- ISR (In-Sync Replicas) больше не может быть сформирован

Kafka в данном случае корректно предпочитает **Consistency**, а не **Availability**.

---

### 4.4. Восстановление брокеров

После восстановления одного из остановленных брокеров:
- кворум восстанавливается
- кластер снова принимает сообщения
- ошибки у продюсера исчезают

После восстановления всех брокеров:
- кластер полностью стабилизируется
- реплики синхронизируются
- данные остаются консистентными

---

## 5. Выводы

В ходе работы был развернут и протестирован Kafka-кластер в режиме KRaft, а также проверена его масштабируемость и отказоустойчивость.

Основные выводы:
1. Kafka масштабируется за счёт партиций, а не за счёт увеличения мощности одного узла.
2. Репликация обеспечивает сохранность данных и отказоустойчивость.
3. Кластер корректно переживает отказ одного брокера без потери доступности.
4. При потере кворума Kafka останавливает запись, сохраняя консистентность данных.
5. Kafka ориентирована на высокую пропускную способность и стабильную работу в распределённых системах.

Таким образом, Kafka является надёжным решением для передачи критичных событий и хорошо демонстрирует принципы распределённых систем и компромисс CAP.
